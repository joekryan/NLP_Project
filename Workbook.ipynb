{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, precision_score, accuracy_score,f1_score, recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import names\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...      10\n",
       "1  My brother is in the market for a high-perform...       3\n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...      17\n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3\n",
       "4  1)    I have an old Jasmine drive which I cann...       4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "data = pd.Series(newsgroups.data)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['text'] + df.columns.tolist()[1:]\n",
    "df['target'] = pd.Series(newsgroups.target)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataframe:  (18846, 2)\n",
      "number of target variables:  20\n",
      "null target variables:  False\n",
      "null text:  False\n"
     ]
    }
   ],
   "source": [
    "print('shape of dataframe: ', df.shape)\n",
    "print('number of target variables: ',df.target.nunique())\n",
    "print('null target variables: ', df.target.isna().any())\n",
    "print('null text: ',df.text.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        \\n\\nI am sure some bashers of Pens fans are pr...\n",
       "1        My brother is in the market for a high-perform...\n",
       "2        \\n\\n\\n\\n\\tFinally you said what you dream abou...\n",
       "3        \\nThink!\\n\\nIt's the SCSI card doing the DMA t...\n",
       "4        1)    I have an old Jasmine drive which I cann...\n",
       "                               ...                        \n",
       "18841    DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...\n",
       "18842    \\nNot in isolated ground recepticles (usually ...\n",
       "18843    I just installed a DX2-66 CPU in a clone mothe...\n",
       "18844    \\nWouldn't this require a hyper-sphere.  In 3-...\n",
       "18845    After a tip from Gary Crum (crum@fcom.cc.utah....\n",
       "Name: text, Length: 18846, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>\\nNot in isolated ground recepticles (usually ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18837 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target\n",
       "0      \\n\\nI am sure some bashers of Pens fans are pr...      10\n",
       "1      My brother is in the market for a high-perform...       3\n",
       "2      \\n\\n\\n\\n\\tFinally you said what you dream abou...      17\n",
       "3      \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3\n",
       "4      1)    I have an old Jasmine drive which I cann...       4\n",
       "...                                                  ...     ...\n",
       "18841  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...      13\n",
       "18842  \\nNot in isolated ground recepticles (usually ...      12\n",
       "18843  I just installed a DX2-66 CPU in a clone mothe...       3\n",
       "18844  \\nWouldn't this require a hyper-sphere.  In 3-...       1\n",
       "18845  After a tip from Gary Crum (crum@fcom.cc.utah....       7\n",
       "\n",
       "[18837 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[df.text != ' ']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f36cb0c2e50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(10,6)})\n",
    "df['target'].hist(bins=df.target.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove emails\n",
    "df['text'] = df['text'].apply(lambda row: re.sub(r\"\\S*@\\S*\\s?\",\"\",row))\n",
    "\n",
    "#remove extra spaces\n",
    "df['text'] = df['text'].apply(lambda row: re.sub(r\"\\s+\",\" \",row))\n",
    "\n",
    "# #remove single quote marks\n",
    "df['text'] = df['text'].apply(lambda row: re.sub(r\"\\'\",\"\",row))\n",
    "\n",
    "#make all text lower case\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "#remove empty rows\n",
    "# df['text'] = df[df.text != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      False\n",
       "target    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16540</th>\n",
       "      <td>article #61153 (61302 is last): from: (nicki a...</td>\n",
       "      <td>3</td>\n",
       "      <td>[article, 61153, 61302, last, nicki, stassen, ...</td>\n",
       "      <td>article 61153 61302 last nicki stassen lantz s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16183</th>\n",
       "      <td>wrong. quite true. and evolution made \"decide...</td>\n",
       "      <td>18</td>\n",
       "      <td>[wrong, quite, true, evolution, made, decided,...</td>\n",
       "      <td>wrong quite true evolution made decided homose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>lord, i hope you dont hoover was a pro! he was...</td>\n",
       "      <td>16</td>\n",
       "      <td>[lord, hope, dont, hoover, pro, monstrous]</td>\n",
       "      <td>lord hope dont hoover pro monstrous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>detroit is a very disciplined team. theres a l...</td>\n",
       "      <td>10</td>\n",
       "      <td>[detroit, disciplined, team, theres, lot, euro...</td>\n",
       "      <td>detroit disciplined team there lot european de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>this past week ive been playing with some of t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[past, week, ive, playing, r, reaction, diffus...</td>\n",
       "      <td>past week ive playing r reaction diffusion con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target  \\\n",
       "16540  article #61153 (61302 is last): from: (nicki a...       3   \n",
       "16183   wrong. quite true. and evolution made \"decide...      18   \n",
       "2888   lord, i hope you dont hoover was a pro! he was...      16   \n",
       "3985   detroit is a very disciplined team. theres a l...      10   \n",
       "10467  this past week ive been playing with some of t...       1   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "16540  [article, 61153, 61302, last, nicki, stassen, ...   \n",
       "16183  [wrong, quite, true, evolution, made, decided,...   \n",
       "2888          [lord, hope, dont, hoover, pro, monstrous]   \n",
       "3985   [detroit, disciplined, team, theres, lot, euro...   \n",
       "10467  [past, week, ive, playing, r, reaction, diffus...   \n",
       "\n",
       "                                              lemmatized  \n",
       "16540  article 61153 61302 last nicki stassen lantz s...  \n",
       "16183  wrong quite true evolution made decided homose...  \n",
       "2888                 lord hope dont hoover pro monstrous  \n",
       "3985   detroit disciplined team there lot european de...  \n",
       "10467  past week ive playing r reaction diffusion con...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instatiate tokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "#tokenize test\n",
    "df['tokenized_text'] = df.apply(lambda row: tokenizer.tokenize(row['text']),axis=1)\n",
    "\n",
    "#define stop words\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "#remove stop words\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "#instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#lemmatize text\n",
    "df['lemmatized'] = df['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "df.lemmatized = df.lemmatized.apply(lambda x: \" \".join(x) )\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tokenized_text.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test validation split\n",
    "\n",
    "X,y = df.lemmatized,df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = make_scorer(accuracy_score, average='macro')\n",
    "precision = make_scorer(precision_score, average='macro')\n",
    "recall =  make_scorer(recall_score, average='macro')\n",
    "f1 = make_scorer(f1_score, average='macro')\n",
    "scoring={'accuracy':accuracy,'precision':precision,'recall':recall,'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pipeline for logistic regression\n",
    "\n",
    "pipe_logreg = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "parameters_logreg = {\n",
    "    'tfidf__min_df': [0.001,0.005,0.01],\n",
    "    'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidf__max_features': [None, 5000, 10000, 50000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "    \"clf__C\": [0.01, 0.1, 1,10],\n",
    "    \"clf__class_weight\": ['balanced'],\n",
    "    \"clf__solver\": ['newton-cg', 'lbfgs', 'sag'],\n",
    "#     \"clf__l1_ratio\":[0,0.2,0.4,0.6,0.8,1],\n",
    "    \"clf__multi_class\":['multinomial']\n",
    "}\n",
    "\n",
    "gs_logreg = GridSearchCV(estimator=pipe_logreg,\n",
    "            param_grid=parameters_logreg,\n",
    "            scoring='accuracy',\n",
    "            cv=KFold(5,shuffle=True,random_state=42), \n",
    "            return_train_score = True, verbose=1,n_jobs=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 30.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 60.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 84.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 91.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.708\n",
      "\n",
      "Best params:\n",
      " {'clf__C': 1, 'clf__class_weight': 'balanced', 'clf__multi_class': 'multinomial', 'clf__solver': 'newton-cg', 'tfidf__max_df': 0.5, 'tfidf__max_features': None, 'tfidf__min_df': 0.001, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# Fit using grid search\n",
    "best_model = gs_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_logreg.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_logreg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'tfidf', 'clf', 'tfidf__analyzer', 'tfidf__binary', 'tfidf__decode_error', 'tfidf__dtype', 'tfidf__encoding', 'tfidf__input', 'tfidf__lowercase', 'tfidf__max_df', 'tfidf__max_features', 'tfidf__min_df', 'tfidf__ngram_range', 'tfidf__norm', 'tfidf__preprocessor', 'tfidf__smooth_idf', 'tfidf__stop_words', 'tfidf__strip_accents', 'tfidf__sublinear_tf', 'tfidf__token_pattern', 'tfidf__tokenizer', 'tfidf__use_idf', 'tfidf__vocabulary', 'clf__C', 'clf__class_weight', 'clf__dual', 'clf__fit_intercept', 'clf__intercept_scaling', 'clf__l1_ratio', 'clf__max_iter', 'clf__multi_class', 'clf__n_jobs', 'clf__penalty', 'clf__random_state', 'clf__solver', 'clf__tol', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_logreg.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1536 candidates, totalling 7680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   33.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.8min\n",
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 39.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 50.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 64.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 80.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 98.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7680 out of 7680 | elapsed: 104.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.400\n",
      "\n",
      "Best params:\n",
      " {'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__max_features': None, 'clf__min_samples_split': 100, 'clf__splitter': 'best', 'tfidf__min_df': 0.001, 'tfidf__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "#pipeline for decision tree\n",
    "\n",
    "pipe_tree = Pipeline([\n",
    "     ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "parameters_tree = {\n",
    "    'tfidf__min_df': [0.001,0.005,0.01],\n",
    "#     'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "#     'tfidf__max_features': [None, 5000, 10000, 50000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"clf__splitter\": ['best','random'],\n",
    "    \"clf__max_features\": [None, 5000, 10000, 50000],\n",
    "    \"clf__max_depth\":[10,20,40,50],\n",
    "    \"clf__min_samples_split\" : [25,50,75,100]\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(estimator=pipe_tree,\n",
    "            param_grid=parameters_tree,\n",
    "            scoring='accuracy',\n",
    "            cv=KFold(5,shuffle=True,random_state=42), \n",
    "            return_train_score = True, verbose=1,n_jobs=-1)\n",
    "\n",
    "# Fit using grid search\n",
    "best_model = gs_tree.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_tree.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'tfidf', 'clf', 'tfidf__analyzer', 'tfidf__binary', 'tfidf__decode_error', 'tfidf__dtype', 'tfidf__encoding', 'tfidf__input', 'tfidf__lowercase', 'tfidf__max_df', 'tfidf__max_features', 'tfidf__min_df', 'tfidf__ngram_range', 'tfidf__norm', 'tfidf__preprocessor', 'tfidf__smooth_idf', 'tfidf__stop_words', 'tfidf__strip_accents', 'tfidf__sublinear_tf', 'tfidf__token_pattern', 'tfidf__tokenizer', 'tfidf__use_idf', 'tfidf__vocabulary', 'clf__ccp_alpha', 'clf__class_weight', 'clf__criterion', 'clf__max_depth', 'clf__max_features', 'clf__max_leaf_nodes', 'clf__min_impurity_decrease', 'clf__min_impurity_split', 'clf__min_samples_leaf', 'clf__min_samples_split', 'clf__min_weight_fraction_leaf', 'clf__presort', 'clf__random_state', 'clf__splitter'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_tree.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 24.2min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 62.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 122.3min\n"
     ]
    }
   ],
   "source": [
    "#pipeline for random forest\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "     ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "parameters_rf = {\n",
    "    'tfidf__min_df': [0.001,0.005,0.01],\n",
    "#     'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "#     'tfidf__max_features': [None, 5000, 10000, 50000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"clf__max_features\": [None, 5000, 10000, 50000],\n",
    "    \"clf__max_depth\":[10,20,40,50],\n",
    "    \"clf__min_samples_split\" : [25,50,75,100]\n",
    "}\n",
    "\n",
    "rf_tree = GridSearchCV(estimator=pipe_rf,\n",
    "            param_grid=parameters_rf,\n",
    "            scoring='accuracy',\n",
    "            cv=KFold(5,shuffle=True,random_state=42), \n",
    "            return_train_score = True, verbose=1,n_jobs=-1)\n",
    "\n",
    "# Fit using grid search\n",
    "best_model = rf_tree.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % rf_tree.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', rf_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.0.2-py3-none-manylinux1_x86_64.whl (109.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 109.7 MB 9.3 kB/s  eta 0:00:01  |█▊                              | 5.8 MB 4.9 MB/s eta 0:00:22     |██                              | 7.0 MB 4.9 MB/s eta 0:00:22     |███▍                            | 11.7 MB 4.9 MB/s eta 0:00:21     |███▉                            | 13.0 MB 4.9 MB/s eta 0:00:20     |████▏                           | 14.3 MB 4.9 MB/s eta 0:00:20     |████▊                           | 16.2 MB 4.9 MB/s eta 0:00:20     |██████                          | 20.7 MB 4.9 MB/s eta 0:00:19     |█████████                       | 31.1 MB 42.1 MB/s eta 0:00:02     |███████████████▊                | 54.1 MB 42.1 MB/s eta 0:00:02     |█████████████████▌              | 59.9 MB 25.9 MB/s eta 0:00:02     |█████████████████▉              | 61.2 MB 25.9 MB/s eta 0:00:02     |████████████████████            | 68.4 MB 25.9 MB/s eta 0:00:02     |████████████████████▋           | 70.9 MB 25.9 MB/s eta 0:00:02     |█████████████████████           | 72.0 MB 25.9 MB/s eta 0:00:02     |██████████████████████▎         | 76.5 MB 25.9 MB/s eta 0:00:02     |██████████████████████▊         | 77.7 MB 25.9 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.18.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
