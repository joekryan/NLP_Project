{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, precision_score, accuracy_score,f1_score, recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import names\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...      10\n",
       "1  My brother is in the market for a high-perform...       3\n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...      17\n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3\n",
       "4  1)    I have an old Jasmine drive which I cann...       4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "data = pd.Series(newsgroups.data)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['text'] + df.columns.tolist()[1:]\n",
    "df['target'] = pd.Series(newsgroups.target)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataframe:  (18846, 2)\n",
      "number of target variables:  20\n",
      "null target variables:  False\n",
      "null text:  False\n"
     ]
    }
   ],
   "source": [
    "print('shape of dataframe: ', df.shape)\n",
    "print('number of target variables: ',df.target.nunique())\n",
    "print('null target variables: ', df.target.isna().any())\n",
    "print('null text: ',df.text.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove emails\n",
    "df['text'] = df['text'].apply(lambda row: re.sub(r\"\\S*@\\S*\\s?\",\"\",row))\n",
    "\n",
    "#remove extra spaces\n",
    "df['text'] = df['text'].apply(lambda row: re.sub(r\"\\s+\",\" \",row))\n",
    "\n",
    "# #remove single quote marks\n",
    "df['text'] = df['text'].apply(lambda row: re.sub(r\"\\'\",\"\",row))\n",
    "\n",
    "#make all text lower case\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>sorry, but i just wanted to be the first hypoc...</td>\n",
       "      <td>18</td>\n",
       "      <td>[sorry, wanted, first, hypocrite, say, hope, i...</td>\n",
       "      <td>sorry wanted first hypocrite say hope im late ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>ive been using version 2.5.2 of ghostscript, a...</td>\n",
       "      <td>2</td>\n",
       "      <td>[ive, using, version, 2, 5, 2, ghostscript, im...</td>\n",
       "      <td>ive using version 2 5 2 ghostscript im quite s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>[...] note that i _never_ said that depressio...</td>\n",
       "      <td>0</td>\n",
       "      <td>[note, never, said, depression, destruction, n...</td>\n",
       "      <td>note never said depression destruction nuclear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14641</th>\n",
       "      <td>this is just basicly right. the connections t...</td>\n",
       "      <td>12</td>\n",
       "      <td>[basicly, right, connections, cannot, handle, ...</td>\n",
       "      <td>basicly right connection cannot handle touchto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11739</th>\n",
       "      <td>hi! anyone knows how can i change an icon fore...</td>\n",
       "      <td>2</td>\n",
       "      <td>[hi, anyone, knows, change, icon, forever, eve...</td>\n",
       "      <td>hi anyone know change icon forever ever mean p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target  \\\n",
       "10706  sorry, but i just wanted to be the first hypoc...      18   \n",
       "5306   ive been using version 2.5.2 of ghostscript, a...       2   \n",
       "9118    [...] note that i _never_ said that depressio...       0   \n",
       "14641   this is just basicly right. the connections t...      12   \n",
       "11739  hi! anyone knows how can i change an icon fore...       2   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "10706  [sorry, wanted, first, hypocrite, say, hope, i...   \n",
       "5306   [ive, using, version, 2, 5, 2, ghostscript, im...   \n",
       "9118   [note, never, said, depression, destruction, n...   \n",
       "14641  [basicly, right, connections, cannot, handle, ...   \n",
       "11739  [hi, anyone, knows, change, icon, forever, eve...   \n",
       "\n",
       "                                              lemmatized  \n",
       "10706  sorry wanted first hypocrite say hope im late ...  \n",
       "5306   ive using version 2 5 2 ghostscript im quite s...  \n",
       "9118   note never said depression destruction nuclear...  \n",
       "14641  basicly right connection cannot handle touchto...  \n",
       "11739  hi anyone know change icon forever ever mean p...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instatiate tokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "#tokenize test\n",
    "df['tokenized_text'] = df.apply(lambda row: tokenizer.tokenize(row['text']),axis=1)\n",
    "\n",
    "#define stop words\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "#remove stop words\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "#instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#lemmatize text\n",
    "df['lemmatized'] = df['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "df.lemmatized = df.lemmatized.apply(lambda x: \" \".join(x) )\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tokenized_text.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test validation split\n",
    "\n",
    "X,y = df.lemmatized,df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.8min finished\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1501: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  \"(penalty={})\".format(self.penalty))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.708\n",
      "\n",
      "Best params:\n",
      " {'clf__C': 1, 'clf__class_weight': 'balanced', 'clf__l1_ratio': 0, 'clf__multi_class': 'multinomial', 'clf__solver': 'newton-cg', 'tfidf__max_df': 0.5, 'tfidf__min_df': 0.001, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "#pipeline for logistic regression\n",
    "\n",
    "pipe_logreg = Pipeline([\n",
    "     ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "parameters_logreg = {\n",
    "    'tfidf__min_df': [0.001],\n",
    "    'tfidf__max_df': [0.5],\n",
    "#     'tfidf__max_features': [None, 5000, 10000, 50000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "    \"clf__C\": [1],\n",
    "    \"clf__class_weight\": ['balanced'],\n",
    "    \"clf__solver\": ['newton-cg'],\n",
    "    \"clf__l1_ratio\":[0,0.2,0.4,0.6,0.8,1],\n",
    "    \"clf__multi_class\":['multinomial']\n",
    "}\n",
    "\n",
    "gs_logreg = GridSearchCV(estimator=pipe_logreg,\n",
    "            param_grid=parameters_logreg,\n",
    "            scoring='accuracy',\n",
    "            cv=KFold(5,shuffle=True,random_state=42), \n",
    "            return_train_score = True, verbose=1,n_jobs=-1)\n",
    "\n",
    "# Fit using grid search\n",
    "best_model = gs_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_logreg.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_logreg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.0min\n"
     ]
    }
   ],
   "source": [
    "#pipeline for decision tree\n",
    "\n",
    "pipe_tree = Pipeline([\n",
    "     ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "parameters_tree = {\n",
    "    'tfidf__min_df': [0.001],\n",
    "    'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"clf__max_depth\":[50,75,100],\n",
    "    \"clf__min_samples_split\" : [100,150,200]\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(estimator=pipe_tree,\n",
    "            param_grid=parameters_tree,\n",
    "            scoring='accuracy',\n",
    "            cv=KFold(5,shuffle=True,random_state=42), \n",
    "            return_train_score = True, verbose=1,n_jobs=-1)\n",
    "\n",
    "# Fit using grid search\n",
    "best_model = gs_tree.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_tree.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for random forest\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "     ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "parameters_rf = {\n",
    "    'tfidf__min_df': [0.001],\n",
    "    'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"clf__max_depth\":[50,75,100],\n",
    "    \"clf__min_samples_split\" : [100,150,200]\n",
    "}\n",
    "\n",
    "gs_rf = GridSearchCV(estimator=pipe_rf,\n",
    "            param_grid=parameters_rf,\n",
    "            scoring='accuracy',\n",
    "            cv=KFold(5,shuffle=True,random_state=42), \n",
    "            return_train_score = True, verbose=1,n_jobs=-1)\n",
    "\n",
    "# Fit using grid search\n",
    "best_model = gs_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_rf.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for NaiveBayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for xgboost\n",
    "\n",
    "pipe_xgb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svd',   TruncatedSVD(algorithm='randomized')),\n",
    "    ('clf',   XGBClassifier(objective='multi:softmax', n_estimators=500, num_class=20)),\n",
    "])\n",
    "\n",
    "parameters_xgb = {\n",
    "    'tfidf__min_df': [0.001],\n",
    "    'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or \n",
    "    \"svd__n_components\": [250,500,750,1000],\n",
    "    \"clf__n_estimators\": [500,1000,5000],\n",
    "    \"clf__learning_rate\":[0.01,0.1,1],\n",
    "    \"clf__min_samples_split\" : [100,150,200]\n",
    "}\n",
    "\n",
    "gs_xgb = GridSearchCV(estimator=pipe_xgb,\n",
    "            param_grid=parameters_xgb,\n",
    "            scoring='accuracy',\n",
    "            cv=KFold(5,shuffle=True,random_state=42), \n",
    "            return_train_score = True, verbose=1,n_jobs=-1)\n",
    "\n",
    "# Fit using grid search\n",
    "best_model = gs_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_xgb.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('Best accuracy: %.3f' % gs_xgb.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
